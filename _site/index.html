<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Survey of Multi-Agent Reinforcement Learning Algorithms | multi-agent-algos</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Survey of Multi-Agent Reinforcement Learning Algorithms" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Mini-project #1 for CMSC 818B: Decision-Making for Robotics (F24)" />
<meta property="og:description" content="Mini-project #1 for CMSC 818B: Decision-Making for Robotics (F24)" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="multi-agent-algos" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Survey of Multi-Agent Reinforcement Learning Algorithms" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"Mini-project #1 for CMSC 818B: Decision-Making for Robotics (F24)","headline":"Survey of Multi-Agent Reinforcement Learning Algorithms","name":"multi-agent-algos","url":"http://localhost:4000/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="icon" type="image/svg+xml" href="./_layouts/github-mark.svg">
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=bcddeace9ba0da2c8b0bc8981d468a22744c5348">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Survey of Multi-Agent Reinforcement Learning Algorithms</h1>
      <h2 class="project-tagline">Mini-project #1 for CMSC 818B: Decision-Making for Robotics (F24)</h2>
      <!-- 
        <a href="https://github.com/jackrcole/multi-agent-algos" class="btn">View on GitHub</a>
      
       -->
    </header>

    <main id="content" class="main-content" role="main">
      <p><a href="/possibilities.html">Link to page of what’s possible</a>.</p>

<h1 id="introduction-to-marl">Introduction to MARL</h1>

<p>Multi-Agent Reinforcement Learning (MARL) is an extension of traditional Reinforcement Learning (RL) that deals with multiple agents learning to interact in a shared environment [1]. Here’s an overview of MARL:</p>

<ol>
  <li><strong>Primer on normal RL</strong>:
    <ul>
      <li>RL involves an agent learning to make decisions by interacting with an environment.</li>
      <li>The agent receives rewards or penalties based on its actions and aims to maximize cumulative rewards [2].</li>
    </ul>
  </li>
  <li><strong>Why do we need MARL?</strong>:
    <ul>
      <li>Real-world scenarios often involve multiple decision-makers or agents.</li>
      <li>MARL allows for modeling complex interactions and dependencies between agents [3].</li>
    </ul>
  </li>
  <li><strong>What necessitates MARL?</strong>:
    <ul>
      <li>Scenarios with multiple autonomous entities (e.g., robotics, game theory, traffic control).</li>
      <li>Problems where decentralized decision-making is crucial [4].</li>
    </ul>
  </li>
  <li><strong>How does MARL distinguish itself from traditional RL?</strong>:
    <ul>
      <li>Multiple agents learning simultaneously.</li>
      <li>Non-stationary environments due to changing behaviors of other agents.</li>
      <li>Potential for cooperation, competition, or mixed scenarios [5].</li>
    </ul>
  </li>
</ol>

<h2 id="types-of-marl">Types of MARL</h2>
<p>MARL algorithms can be broadly categorized into two types: On-line and Off-line.</p>

<h3 id="on-line-marl">On-Line MARL</h3>
<p>On-line MARL algorithms learn from continuous interactions with the environment. Some examples of On-Line MARL algorithms include:</p>

<ol>
  <li><strong>Value Decomposition Networks (VDN)</strong>:
    <ul>
      <li>VDN decomposes the team value function into a sum of individual agent value functions [6].</li>
      <li>It assumes that the global Q-function can be additively decomposed into individual agent Q-functions.</li>
      <li>This approach allows for decentralized execution with centralized training.</li>
    </ul>
  </li>
  <li><strong>QMIX</strong>:
    <ul>
      <li>QMIX extends VDN by using a mixing network to combine individual agent Q-values [7].</li>
      <li>It allows for a more complex relationship between individual and team value functions.</li>
      <li>QMIX ensures that a global argmax performed on the joint action-value function yields the same result as a set of individual argmax operations performed on each agent’s Q-values.</li>
    </ul>
  </li>
  <li><strong>Multi-Agent DDPG (MADDPG)</strong>:
    <ul>
      <li>MADDPG is an extension of DDPG for multi-agent scenarios [8].</li>
      <li>It uses a centralized training with decentralized execution paradigm.</li>
      <li>Each agent has its own actor and critic, where the critic has access to all agents’ observations and actions during training.</li>
    </ul>
  </li>
</ol>

<h3 id="off-line-marl">Off-Line MARL</h3>
<p>Off-line MARL algorithms learn from a fixed dataset of experiences without direct interaction with the environment. Some examples of Off-line MARL algorithms include:</p>

<ol>
  <li><strong>BCQ for Multi-Agent RL (MA-BCQ)</strong>:
    <ul>
      <li>MA-BCQ adapts the single-agent Batch Constrained Q-learning (BCQ) to multi-agent settings [9].</li>
      <li>It addresses the extrapolation error in offline RL by constraining the learned policy to be close to the behavior policy in the dataset.</li>
    </ul>
  </li>
  <li><strong>Multi-Agent Constrained Policy Optimization (MACPO)</strong>:
    <ul>
      <li>MACPO extends Constrained Policy Optimization to multi-agent scenarios for offline learning [10].</li>
      <li>It uses a trust region approach to ensure policy improvement while satisfying constraints.</li>
    </ul>
  </li>
  <li><strong>Offline Multi-Agent Reinforcement Learning with Implicit Constraint (OMAIC)</strong>:
    <ul>
      <li>OMAIC tackles the challenge of distribution shift in offline MARL [11].</li>
      <li>It introduces an implicit constraint to penalize out-of-distribution actions and encourages in-distribution actions.</li>
    </ul>
  </li>
</ol>

<h1 id="time-permitting-in-depth-exploration">(Time Permitting) In-Depth Exploration</h1>
<p>Explore 1 Algorithm from above in-depth</p>

<h1 id="time-permitting-current-sota">(Time Permitting) Current SOTA</h1>
<p>Find some papers from recent conferences and talk about what is the current SOTA</p>

<h1 id="references">References</h1>

<p>[1] Buşoniu, L., Babuška, R., &amp; De Schutter, B. (2010). Multi-agent reinforcement learning: An overview. In Innovations in multi-agent systems and applications-1 (pp. 183-221). Springer.</p>

<p>[2] Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.</p>

<p>[3] Zhang, K., Yang, Z., &amp; Başar, T. (2021). Multi-agent reinforcement learning: A selective overview of theories and algorithms. Handbook of Reinforcement Learning and Control, 321-384.</p>

<p>[4] Hernandez-Leal, P., Kartal, B., &amp; Taylor, M. E. (2019). A survey and critique of multiagent deep reinforcement learning. Autonomous Agents and Multi-Agent Systems, 33(6), 750-797.</p>

<p>[5] Gronauer, S., &amp; Diepold, K. (2022). Multi-agent deep reinforcement learning: a survey. Artificial Intelligence Review, 55(2), 895-943.</p>

<p>[6] Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W. M., Zambaldi, V., Jaderberg, M., … &amp; Graepel, T. (2018). Value-decomposition networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (pp. 2085-2087).</p>

<p>[7] Rashid, T., Samvelyan, M., Schroeder, C., Farquhar, G., Foerster, J., &amp; Whiteson, S. (2018). QMIX: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning (pp. 4295-4304). PMLR.</p>

<p>[8] Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., &amp; Mordatch, I. (2017). Multi-agent actor-critic for mixed cooperative-competitive environments. In Advances in neural information processing systems (pp. 6379-6390).</p>

<p>[9] Yang, Y., Tutunov, R., Sakulwongtana, P., &amp; Ammar, H. B. (2021). Project-based multi-agent reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (pp. 1527-1535).</p>

<p>[10] Yang, Y., Luo, Y., Li, M., Schuurmans, D., &amp; Ammar, H. B. (2022). Batch reinforcement learning with hyperparameter gradients. In International Conference on Machine Learning (pp. 24983-25009). PMLR.</p>

<p>[11] Jiang, M., Dmitriev, A., &amp; Kuhnle, A. (2022). Offline multi-agent reinforcement learning with implicit constraint. arXiv preprint arXiv:2209.11698.</p>


      <footer class="site-footer">
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>